{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4d2e6-6adc-428b-9058-30659a01950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import yfinance as yf\n",
    "\n",
    "def load_stock_data(file_path, num_days=365):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';', decimal=',', thousands='.', \n",
    "                         encoding='utf-8', skiprows=12)\n",
    "        \n",
    "        df = df.replace(['', '%'], np.nan).infer_objects(copy=False)\n",
    "        df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "        \n",
    "        month_map = {\n",
    "            'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,\n",
    "            'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12\n",
    "        }\n",
    "        \n",
    "        def convert_date(date_str):\n",
    "            try:\n",
    "                if pd.isna(date_str):\n",
    "                    return None\n",
    "                month, year = str(date_str).lower().strip().split('-')\n",
    "                month_num = month_map.get(month.strip(), 1)\n",
    "                return pd.Timestamp(2000 + int(year), month_num, 1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting date {date_str}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        df['Dates'] = df.iloc[:,0].apply(convert_date)\n",
    "        df = df.dropna(subset=['Dates'])\n",
    "        \n",
    "        df.set_index('Dates', inplace=True)\n",
    "        \n",
    "        cols = {\n",
    "            'PX_OPEN': np.nan,\n",
    "            'PX_HIGH': np.nan,\n",
    "            'PX_LOW': np.nan,\n",
    "            'PX_LAST': np.nan,\n",
    "            'PX_VOLUME': np.nan,\n",
    "            \n",
    "            'PX_TO_BOOK_RATIO': pd.to_numeric(df.iloc[:,7], errors='coerce'),\n",
    "            'BS_CUR_LIAB': pd.to_numeric(df.iloc[:,25], errors='coerce'),\n",
    "            'CUR_MKT_CAP': pd.to_numeric(df.iloc[:,2], errors='coerce'),\n",
    "            'EBITDA': pd.to_numeric(df.iloc[:,17], errors='coerce'),\n",
    "            'NET_INCOME': pd.to_numeric(df.iloc[:,18], errors='coerce'),\n",
    "            'PE_RATIO': pd.to_numeric(df.iloc[:,6], errors='coerce'),\n",
    "            'EV_EBITDA': pd.to_numeric(df.iloc[:,8], errors='coerce'),\n",
    "            'DEBT_TO_EBITDA': pd.to_numeric(df.iloc[:,30], errors='coerce'),\n",
    "            'GROSS_MARGIN': pd.to_numeric(df.iloc[:,14], errors='coerce'),\n",
    "            'EBIT_MARGIN': pd.to_numeric(df.iloc[:,15], errors='coerce'),\n",
    "            'LIQ_RATIO': pd.to_numeric(df.iloc[:,29], errors='coerce'),\n",
    "            'ROE': pd.to_numeric(df.iloc[:,18], errors='coerce'),\n",
    "            'ROA': pd.to_numeric(df.iloc[:,18], errors='coerce') / pd.to_numeric(df.iloc[:,24], errors='coerce'),\n",
    "            'ASSET_TURNOVER': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'CAPEX': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'FCF': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'ROIC': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'NET_DEBT': pd.to_numeric(df.iloc[:,1], errors='coerce')\n",
    "        }\n",
    "        \n",
    "        processed_df = pd.DataFrame(cols, index=df.index)\n",
    "        \n",
    "        # Determinar o primeiro e último índice válido dos fundamentos\n",
    "        first_valid = processed_df.apply(lambda x: x.first_valid_index()).min()\n",
    "        last_valid = processed_df.apply(lambda x: x.last_valid_index()).max()\n",
    "        \n",
    "        if first_valid is None or last_valid is None:\n",
    "            ticker = os.path.basename(file_path).replace('.csv', '')\n",
    "            print(f\"Excluindo {ticker}: Nenhum dado válido encontrado\")\n",
    "            return None\n",
    "        \n",
    "        processed_df = processed_df[first_valid:last_valid]\n",
    "        \n",
    "        missing_percentage = (processed_df.isna().sum().sum() / \n",
    "                              (processed_df.shape[0] * processed_df.shape[1])) * 100\n",
    "        \n",
    "        ticker = os.path.basename(file_path).replace('.csv', '')\n",
    "        print(f\"\\nAnálise do ativo {ticker}:\")\n",
    "        print(f\"Período de dados fundamentas disponível: {first_valid.strftime('%Y-%m-%d')} até {last_valid.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Total de {(last_valid - first_valid).days / 365.25:.1f} anos de histórico fundamental\")\n",
    "        print(f\"Porcentagem de valores faltantes (fundamentais): {missing_percentage:.2f}%\")\n",
    "        \n",
    "        if missing_percentage > 30:\n",
    "            print(f\"Excluindo {ticker}: {missing_percentage:.2f}% de valores faltantes\")\n",
    "            return None\n",
    "        \n",
    "        # Ajustando a data final para ontem\n",
    "        end_yahoo = pd.Timestamp.today() - pd.Timedelta(days=1)\n",
    "        # Ajustando a data inicial baseada na quantidade de dias escolhida\n",
    "        start_yahoo = end_yahoo - pd.Timedelta(days=num_days)\n",
    "        \n",
    "        # Adicionando o sufixo .SA para tickers brasileiros\n",
    "        ticker_yahoo = ticker.upper() + '.SA'\n",
    "        \n",
    "        try:\n",
    "            yahoo_data = yf.download(ticker_yahoo, start=start_yahoo, end=end_yahoo)\n",
    "            if yahoo_data.empty:\n",
    "                print(f\"Não foi possível obter dados do Yahoo para {ticker}. Excluindo.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar dados do Yahoo para {ticker}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Reindexar os fundamentos no índice do yahoo_data\n",
    "        processed_df = processed_df.reindex(yahoo_data.index)\n",
    "        \n",
    "        # Separar colunas de fundamentos para ffill\n",
    "        fundamentals_cols = [c for c in processed_df.columns \n",
    "                             if c not in ['PX_OPEN','PX_HIGH','PX_LOW','PX_LAST','PX_VOLUME']]\n",
    "        \n",
    "        # Forward-fill e backward-fill apenas nas colunas de fundamentos\n",
    "        processed_df[fundamentals_cols] = processed_df[fundamentals_cols].ffill().bfill()\n",
    "        \n",
    "        # Inserir dados de preço do Yahoo\n",
    "        processed_df['PX_OPEN'] = yahoo_data['Open']\n",
    "        processed_df['PX_HIGH'] = yahoo_data['High']\n",
    "        processed_df['PX_LOW'] = yahoo_data['Low']\n",
    "        processed_df['PX_LAST'] = yahoo_data['Close']\n",
    "        processed_df['PX_VOLUME'] = yahoo_data['Volume']\n",
    "        \n",
    "        # Preencher NaN nos fundamentos com a mediana ou zero\n",
    "        for column in fundamentals_cols:\n",
    "            median_value = processed_df[column].median()\n",
    "            if pd.isna(median_value):\n",
    "                processed_df[column] = processed_df[column].fillna(0)\n",
    "            else:\n",
    "                processed_df[column] = processed_df[column].fillna(median_value)\n",
    "        \n",
    "        na_count = processed_df.isna().sum().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"Warning: Found {na_count} NA values in {file_path}\")\n",
    "            fill_dict = {col: 0 for col in fundamentals_cols if processed_df[col].isna().any()}\n",
    "            processed_df = processed_df.fillna(fill_dict)\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_merged_dataset(input_folder='./data/', output_file='fundamentals.csv', num_days=365):\n",
    "    \"\"\"\n",
    "    Process all CSV files and create merged dataset compatible with load_data().\n",
    "    O parâmetro num_days define o período histórico a ser baixado do Yahoo a partir de ontem.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    all_data = {}\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {input_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    skipped_files = 0\n",
    "    for file in csv_files:\n",
    "        ticker = os.path.basename(file).replace('.csv', '')\n",
    "        try:\n",
    "            print(f\"\\nProcessando {ticker}...\")\n",
    "            df = load_stock_data(file, num_days=num_days)\n",
    "            if df is not None and not df.empty:\n",
    "                all_data[f\"{ticker} Index\"] = df\n",
    "                print(f\"Successfully processed {ticker} Index\")\n",
    "                print(f\"DataFrame shape: {df.shape}\")\n",
    "                print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "            else:\n",
    "                print(f\"Ativo {ticker} excluído devido a dados insuficientes\")\n",
    "                skipped_files += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker} Index: {str(e)}\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "    print(f\"\\nResumo do processamento:\")\n",
    "    print(f\"Total de arquivos encontrados: {len(csv_files)}\")\n",
    "    print(f\"Arquivos processados com sucesso: {len(all_data)}\")\n",
    "    print(f\"Arquivos excluídos: {skipped_files}\")\n",
    "    \n",
    "    print(\"\\nMerging all data...\")\n",
    "    merged_df = pd.concat(all_data, axis=1)\n",
    "    \n",
    "    # Ajustar o cabeçalho\n",
    "    merged_df.reset_index(inplace=True)  \n",
    "    merged_df = merged_df.round(2)\n",
    "\n",
    "    first_header = ['Dates'] + [col[0] for col in merged_df.columns[1:]]\n",
    "    second_header = ['Dates'] + [col[1] for col in merged_df.columns[1:]]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(','.join(first_header) + '\\n')\n",
    "        f.write(','.join(second_header) + '\\n')\n",
    "        merged_df.to_csv(f, index=False, header=False)\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to {output_file}\")\n",
    "    print(f\"Total stocks processed: {len(all_data)}\")\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11c584-0ae1-4362-be4a-0a6ea48b03ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Então execute\n",
    "create_merged_dataset(input_folder='./data/', output_file='fundamentals.csv', num_days=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aabc9a-6bde-40e9-9034-0cf0ca3632ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90038597-392b-4d0b-bdae-ee9b89b9bed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
