{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2cbcd4-a856-4e8a-8d6d-a9425b31d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_stock_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and processes individual stock CSV maintaining original structure.\n",
    "    Handles assets with different historical periods.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV without skipfooter, just skip header\n",
    "        df = pd.read_csv(file_path, sep=';', decimal=',', thousands='.', \n",
    "                        encoding='utf-8', skiprows=12)\n",
    "        \n",
    "        # Remove empty rows and columns more precisely\n",
    "        df = df.replace(['', '%'], np.nan).infer_objects(copy=False)\n",
    "        df = df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "        \n",
    "        # Convert dates first to ensure proper alignment\n",
    "        month_map = {\n",
    "            'jan': 1, 'fev': 2, 'mar': 3, 'abr': 4, 'mai': 5, 'jun': 6,\n",
    "            'jul': 7, 'ago': 8, 'set': 9, 'out': 10, 'nov': 11, 'dez': 12\n",
    "        }\n",
    "        \n",
    "        def convert_date(date_str):\n",
    "            try:\n",
    "                if pd.isna(date_str):\n",
    "                    return None\n",
    "                month, year = str(date_str).lower().strip().split('-')\n",
    "                month_num = month_map.get(month.strip(), 1)\n",
    "                return pd.Timestamp(2000 + int(year), month_num, 1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting date {date_str}: {str(e)}\")\n",
    "                return None\n",
    "        \n",
    "        # Convert dates and drop rows with invalid dates\n",
    "        df['Dates'] = df.iloc[:,0].apply(convert_date)\n",
    "        df = df.dropna(subset=['Dates'])\n",
    "        \n",
    "        # Set dates as index before processing other columns\n",
    "        df.set_index('Dates', inplace=True)\n",
    "        \n",
    "        # Define column mappings with proper type conversion\n",
    "        cols = {\n",
    "            'PX_OPEN': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'PX_HIGH': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'PX_LOW': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'PX_LAST': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'PX_VOLUME': pd.to_numeric(df.iloc[:,2], errors='coerce'),\n",
    "            'PX_TO_BOOK_RATIO': pd.to_numeric(df.iloc[:,7], errors='coerce'),\n",
    "            'BS_CUR_LIAB': pd.to_numeric(df.iloc[:,25], errors='coerce'),\n",
    "            'CUR_MKT_CAP': pd.to_numeric(df.iloc[:,2], errors='coerce'),\n",
    "            'EBITDA': pd.to_numeric(df.iloc[:,17], errors='coerce'),\n",
    "            'NET_INCOME': pd.to_numeric(df.iloc[:,18], errors='coerce'),\n",
    "            'PE_RATIO': pd.to_numeric(df.iloc[:,6], errors='coerce'),\n",
    "            'EV_EBITDA': pd.to_numeric(df.iloc[:,8], errors='coerce'),\n",
    "            'DEBT_TO_EBITDA': pd.to_numeric(df.iloc[:,30], errors='coerce'),\n",
    "            'GROSS_MARGIN': pd.to_numeric(df.iloc[:,14], errors='coerce'),\n",
    "            'EBIT_MARGIN': pd.to_numeric(df.iloc[:,15], errors='coerce'),\n",
    "            'LIQ_RATIO': pd.to_numeric(df.iloc[:,29], errors='coerce'),\n",
    "            'ROE': pd.to_numeric(df.iloc[:,18], errors='coerce'),\n",
    "            'ROA': pd.to_numeric(df.iloc[:,18], errors='coerce') / pd.to_numeric(df.iloc[:,24], errors='coerce'),\n",
    "            'ASSET_TURNOVER': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'CAPEX': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'FCF': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'ROIC': pd.to_numeric(df.iloc[:,1], errors='coerce'),\n",
    "            'NET_DEBT': pd.to_numeric(df.iloc[:,1], errors='coerce')\n",
    "        }\n",
    "        \n",
    "        # Create DataFrame using the processed index\n",
    "        processed_df = pd.DataFrame(cols, index=df.index)\n",
    "        \n",
    "        # Identificar o período válido do ativo\n",
    "        # Consideramos como início a primeira data com pelo menos um valor não-NA\n",
    "        first_valid = processed_df.apply(lambda x: x.first_valid_index()).min()\n",
    "        last_valid = processed_df.apply(lambda x: x.last_valid_index()).max()\n",
    "        \n",
    "        if first_valid is None or last_valid is None:\n",
    "            ticker = os.path.basename(file_path).replace('.csv', '')\n",
    "            print(f\"Excluindo {ticker}: Nenhum dado válido encontrado\")\n",
    "            return None\n",
    "        \n",
    "        # Recortar o DataFrame para incluir apenas o período com dados\n",
    "        processed_df = processed_df[first_valid:last_valid]\n",
    "        \n",
    "        # Calcular a porcentagem de valores faltantes apenas no período válido\n",
    "        missing_percentage = (processed_df.isna().sum().sum() / \n",
    "                            (processed_df.shape[0] * processed_df.shape[1])) * 100\n",
    "        \n",
    "        ticker = os.path.basename(file_path).replace('.csv', '')\n",
    "        print(f\"\\nAnálise do ativo {ticker}:\")\n",
    "        print(f\"Período de dados: {first_valid.strftime('%Y-%m-%d')} até {last_valid.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Total de {(last_valid - first_valid).days / 365.25:.1f} anos de histórico\")\n",
    "        print(f\"Porcentagem de valores faltantes: {missing_percentage:.2f}%\")\n",
    "        \n",
    "        # Se tiver mais de 30% de valores faltantes no período válido, retorna None\n",
    "        if missing_percentage > 30:\n",
    "            print(f\"Excluindo {ticker}: {missing_percentage:.2f}% de valores faltantes\")\n",
    "            return None\n",
    "            \n",
    "        # Tratamento de valores faltantes de forma mais segura\n",
    "        # 1. Forward fill\n",
    "        processed_df = processed_df.ffill()\n",
    "        \n",
    "        # 2. Backward fill\n",
    "        processed_df = processed_df.bfill()\n",
    "        \n",
    "        # 3. Preenchimento com medianas - método atualizado\n",
    "        for column in processed_df.columns:\n",
    "            median_value = processed_df[column].median()\n",
    "            if pd.isna(median_value):\n",
    "                processed_df.loc[:, column] = processed_df[column].fillna(0)\n",
    "            else:\n",
    "                processed_df.loc[:, column] = processed_df[column].fillna(median_value)\n",
    "        \n",
    "        # 4. Validação final\n",
    "        na_count = processed_df.isna().sum().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"Warning: Found {na_count} NA values in {file_path}\")\n",
    "            # Último recurso: preencher NA restantes com 0\n",
    "            fill_dict = {col: 0 for col in processed_df.columns if processed_df[col].isna().any()}\n",
    "            processed_df = processed_df.fillna(fill_dict)\n",
    "        \n",
    "        # Agora sim fazemos o resample para daily\n",
    "        processed_df = processed_df.resample('D').ffill()\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Resto do código permanece igual...\n",
    "\n",
    "def create_merged_dataset(input_folder='./csv_files/', output_file='fundamentals.csv'):\n",
    "    \"\"\"\n",
    "    Process all CSV files and create merged dataset compatible with load_data()\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "    all_data = {}\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {input_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    # Process each file\n",
    "    skipped_files = 0\n",
    "    for file in csv_files:\n",
    "        ticker = os.path.basename(file).replace('.csv', '')\n",
    "        try:\n",
    "            print(f\"\\nProcessando {ticker}...\")\n",
    "            df = load_stock_data(file)\n",
    "            if df is not None and not df.empty:\n",
    "                all_data[f\"{ticker} Index\"] = df\n",
    "                print(f\"Successfully processed {ticker} Index\")\n",
    "                print(f\"DataFrame shape: {df.shape}\")\n",
    "                print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "            else:\n",
    "                print(f\"Ativo {ticker} excluído devido a dados insuficientes\")\n",
    "                skipped_files += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker} Index: {str(e)}\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "    print(f\"\\nResumo do processamento:\")\n",
    "    print(f\"Total de arquivos encontrados: {len(csv_files)}\")\n",
    "    print(f\"Arquivos processados com sucesso: {len(all_data)}\")\n",
    "    print(f\"Arquivos excluídos: {skipped_files}\")\n",
    "    \n",
    "    # Create proper MultiIndex DataFrame\n",
    "    print(\"\\nMerging all data...\")\n",
    "    merged_df = pd.concat(all_data, axis=1)\n",
    "    \n",
    "    # Ajustar o cabeçalho para incluir \"Dates\" na primeira coluna\n",
    "    merged_df.reset_index(inplace=True)  # Mover 'Dates' (índice) para colunas\n",
    "\n",
    "    # Configurar o cabeçalho com duas linhas\n",
    "    first_header = ['Dates'] + [col[0] for col in merged_df.columns[1:]]\n",
    "    second_header = ['Dates'] + [col[1] for col in merged_df.columns[1:]]\n",
    "    \n",
    "    # Salvar o CSV com duas linhas de cabeçalho\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Escrever primeira linha do cabeçalho\n",
    "        f.write(','.join(first_header) + '\\n')\n",
    "        # Escrever segunda linha do cabeçalho\n",
    "        f.write(','.join(second_header) + '\\n')\n",
    "        # Escrever os dados\n",
    "        merged_df.to_csv(f, index=False, header=False)\n",
    "    \n",
    "    print(f\"\\nProcessed data saved to {output_file}\")\n",
    "    print(f\"Total stocks processed: {len(all_data)}\")\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94274e85-fe22-41c7-9357-3be79e1b4f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 CSV files\n",
      "\n",
      "Processando Vale3...\n",
      "\n",
      "Análise do ativo Vale3:\n",
      "Período de dados: 2019-12-01 até 2024-09-01\n",
      "Total de 4.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed Vale3 Index\n",
      "DataFrame shape: (1737, 23)\n",
      "Date range: 2019-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando ITUB4...\n",
      "Error processing file ./csv_files/ITUB4.csv: single positional indexer is out-of-bounds\n",
      "Error processing ITUB4 Index: single positional indexer is out-of-bounds\n",
      "\n",
      "Processando BBAS3...\n",
      "Error processing file ./csv_files/BBAS3.csv: single positional indexer is out-of-bounds\n",
      "Error processing BBAS3 Index: single positional indexer is out-of-bounds\n",
      "\n",
      "Processando SUZB3...\n",
      "\n",
      "Análise do ativo SUZB3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed SUZB3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando B3SA3...\n",
      "\n",
      "Análise do ativo B3SA3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed B3SA3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando WEGE3...\n",
      "\n",
      "Análise do ativo WEGE3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed WEGE3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando BBDC4...\n",
      "Error processing file ./csv_files/BBDC4.csv: single positional indexer is out-of-bounds\n",
      "Error processing BBDC4 Index: single positional indexer is out-of-bounds\n",
      "\n",
      "Processando LREN3...\n",
      "\n",
      "Análise do ativo LREN3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed LREN3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando Petr4...\n",
      "\n",
      "Análise do ativo Petr4:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed Petr4 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando RENT3...\n",
      "\n",
      "Análise do ativo RENT3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed RENT3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando PETR3...\n",
      "\n",
      "Análise do ativo PETR3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed PETR3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Processando ABEV3...\n",
      "\n",
      "Análise do ativo ABEV3:\n",
      "Período de dados: 2014-12-01 até 2024-09-01\n",
      "Total de 9.8 anos de histórico\n",
      "Porcentagem de valores faltantes: 0.00%\n",
      "Successfully processed ABEV3 Index\n",
      "DataFrame shape: (3563, 23)\n",
      "Date range: 2014-12-01 00:00:00 to 2024-09-01 00:00:00\n",
      "\n",
      "Resumo do processamento:\n",
      "Total de arquivos encontrados: 12\n",
      "Arquivos processados com sucesso: 9\n",
      "Arquivos excluídos: 3\n",
      "\n",
      "Merging all data...\n",
      "\n",
      "Processed data saved to fundamentals.csv\n",
      "Total stocks processed: 9\n"
     ]
    }
   ],
   "source": [
    "# Então execute\n",
    "merged_data = create_merged_dataset(\n",
    "    input_folder='./csv_files/',  # pasta onde estão seus arquivos CSV\n",
    "    output_file='fundamentals.csv'  # nome do arquivo de saída\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321b430-37a3-43d2-ad38-a95b32d29f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
